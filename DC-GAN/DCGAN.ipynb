{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(os.path.dirname(\"logs/\" + current_time + \"/\"), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(\"models/\" + current_time + \"/\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"gan\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(\"logs/\" + current_time + \"/gan.log\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "sh = logging.StreamHandler()\n",
    "sh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(levelname)s - %(message)s\")\n",
    "fh.setFormatter(formatter)\n",
    "sh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(sh)\n",
    "logger.debug(\"Starting..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 29580), started 0:52:24 ago. (Use '!kill 29580' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-139527233fccb96f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-139527233fccb96f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "logger.debug(\"Starting Tensorboard..\")\n",
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - TensorFlow version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "# from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "import io\n",
    "import gc\n",
    "from packaging import version\n",
    "\n",
    "logger.info(\"TensorFlow version: %s\", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_built_with_cuda():\n",
    "    gpu = tf.test.is_gpu_available()\n",
    "if not gpu:\n",
    "    logger.debug(\"GPU Acceleration not available!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Number of training examples: 60000\n",
      "INFO - Dimension of each example: (28, 28)\n",
      "INFO - Dtype of examples: uint8\n",
      "INFO - Max pixel value: 255\n",
      "INFO - Min pixel value: 0\n"
     ]
    }
   ],
   "source": [
    "logger.debug(\"Using MNIST dataset..\")\n",
    "# (X_train, _), _ = fashion_mnist.load_data()\n",
    "(X_train, _), _ = mnist.load_data()\n",
    "logger.info(\"Number of training examples: %d\", X_train.shape[0])\n",
    "logger.info(\"Dimension of each example: %s\", X_train.shape[1:] if len(X_train.shape)>1 else 1)\n",
    "logger.info(\"Dtype of examples: %s\", X_train.dtype)\n",
    "logger.info(\"Max pixel value: %d\", X_train.max())\n",
    "logger.info(\"Min pixel value: %d\", X_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(\"Global Variables..\")\n",
    "BATCH_SIZE = 32\n",
    "FEAT_DIM = 49\n",
    "EPOCHS = 1000\n",
    "IMG_DIM = (28, 28, 1)\n",
    "DATASET_SIZE = X_train.shape[0]\n",
    "logger.debug(\"Batch Size = %d\", BATCH_SIZE)\n",
    "logger.debug(\"Exanded dimension of images as used for training = %d\", IMG_DIM)\n",
    "logger.debug(\"Dimension of noise input used by generator to generate fakes = %d\", FEAT_DIM)\n",
    "logger.debug(\"Number of training epochs = %d\", EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Range of X_train values: -1 to 1\n",
      "INFO - Shape of training data: 60000 x 28 x 28 x 1\n"
     ]
    }
   ],
   "source": [
    "logger.debug(\"Rescaling training values to lie between -1 and 1 (including both)..\")\n",
    "X_train = X_train.astype(float)\n",
    "X_train = (X_train-127.5)/127.5\n",
    "logger.info(\"Range of X_train values: %d to %d\", X_train.min(), X_train.max())\n",
    "\n",
    "logger.debug(\"Reshaping training data to add depth dimension in images..\")\n",
    "X_train = np.reshape(X_train, (*X_train.shape, 1))\n",
    "logger.info(\"Shape of training data: %d x %d x %d x %d\", X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(\"Mini-Batching and Shuffling dataset..\")\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train)).shuffle(DATASET_SIZE, reshuffle_each_iteration = True).batch(BATCH_SIZE, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d (SpatialDr (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 64)        51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_1 (Spatial (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_2 (Spatial (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_3 (Spatial (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_4 (Spatial (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 1, 256)         1605888   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,918,273\n",
      "Trainable params: 1,917,441\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "logger.debug(\"Defining the model for discriminator..\")\n",
    "des_in = keras.layers.Input(shape = (28, 28, 1))\n",
    "des = keras.layers.Conv2D(32, (5, 5), kernel_initializer = \"random_normal\", padding = \"same\")(des_in)\n",
    "des = keras.layers.BatchNormalization()(des)\n",
    "des = LeakyReLU(0.2)(des)\n",
    "des = keras.layers.SpatialDropout2D(0.3)(des)\n",
    "des = keras.layers.Conv2D(64, (5, 5), kernel_initializer = \"random_normal\", padding = \"same\")(des)\n",
    "des = keras.layers.BatchNormalization()(des)\n",
    "des = LeakyReLU(0.2)(des)\n",
    "des = keras.layers.SpatialDropout2D(0.3)(des)\n",
    "des = keras.layers.AveragePooling2D(pool_size = (2,2), strides = None)(des)\n",
    "des = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"random_normal\", padding = \"same\")(des)\n",
    "des = keras.layers.BatchNormalization()(des)\n",
    "des = LeakyReLU(0.2)(des)\n",
    "des = keras.layers.SpatialDropout2D(0.3)(des)\n",
    "des = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"random_normal\", padding = \"same\")(des)\n",
    "des = keras.layers.BatchNormalization()(des)\n",
    "des = LeakyReLU(0.2)(des)\n",
    "des = keras.layers.SpatialDropout2D(0.3)(des)\n",
    "des = keras.layers.AveragePooling2D(pool_size = (2,2), strides = None)(des)\n",
    "des = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"random_normal\", padding = \"same\")(des)\n",
    "des = keras.layers.BatchNormalization()(des)\n",
    "des = LeakyReLU(0.2)(des)\n",
    "des = keras.layers.SpatialDropout2D(0.3)(des)\n",
    "des = keras.layers.Conv2D(256, (7, 7), kernel_initializer = \"random_normal\", padding = \"valid\")(des)\n",
    "# des = keras.layers.BatchNormalization()(des)\n",
    "des = LeakyReLU(0.2)(des)\n",
    "# des = keras.layers.SpatialDropout2D(0.3)(des)\n",
    "# des = keras.layers.GlobalAveragePooling2D()(des)\n",
    "# des = keras.layers.Conv2D(256, (7, 7), kernel_initializer = \"random_normal\", padding = \"valid\")(des)\n",
    "des = keras.layers.Flatten()(des)\n",
    "des = keras.layers.Dropout(0.3)(des)\n",
    "# des = keras.layers.Dense(FEAT_DIM, kernel_initializer = \"random_normal\")(des)\n",
    "# des = LeakyReLU(0.2)(des)\n",
    "des_out = keras.layers.Dense(1, kernel_initializer = \"random_normal\")(des)\n",
    "discriminator  = keras.Model(inputs = des_in, outputs = des_out)\n",
    "logger.debug(\"Discriminator Summary\")\n",
    "discriminator.summary(print_fn = logger.debug)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 49)]              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 256)         2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_5 (Spatial (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_6 (Spatial (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_7 (Spatial (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_8 (Spatial (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_9 (Spatial (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_10 (Spatia (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 1)         33        \n",
      "=================================================================\n",
      "Total params: 577,089\n",
      "Trainable params: 575,745\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "logger.debug(\"Defining generator model..\")\n",
    "gen_in = keras.layers.Input(shape = (49, ))\n",
    "# gen = keras.layers.Dense(7*7*FEAT_DIM, kernel_initializer = \"random_normal\")(gen_in)\n",
    "# gen = LeakyReLU(0.2)(gen)\n",
    "# gen = keras.layers.Dropout(0.5)(gen)\n",
    "# gen = keras.layers.Reshape((7, 7, FEAT_DIM))(gen)\n",
    "gen = keras.layers.Reshape((7, 7, 1))(gen_in)\n",
    "# gen = keras.layers.Conv2D(256, (3,3), kernel_initializer = \"random_normal\", padding = \"same\")(gen)\n",
    "# gen = LeakyReLU(0.2)(gen)\n",
    "# gen = keras.layers.GaussianNoise(0.01)(gen)\n",
    "gen = keras.layers.Conv2D(256, (3,3), kernel_initializer = \"random_normal\", padding = \"same\")(gen)\n",
    "gen = keras.layers.BatchNormalization()(gen)\n",
    "gen = LeakyReLU(0.2)(gen)\n",
    "gen = keras.layers.SpatialDropout2D(0.3)(gen)\n",
    "gen = keras.layers.Conv2D(128, (3,3), kernel_initializer = \"random_normal\", padding = \"same\")(gen)\n",
    "gen = keras.layers.BatchNormalization()(gen)\n",
    "gen = LeakyReLU(0.2)(gen)\n",
    "gen = keras.layers.SpatialDropout2D(0.3)(gen)\n",
    "gen = keras.layers.UpSampling2D(size = (2,2), interpolation = \"bilinear\")(gen)\n",
    "# gen = keras.layers.GaussianNoise(0.01)(gen)\n",
    "gen = keras.layers.Conv2D(128, (3,3), kernel_initializer = \"random_normal\", padding = \"same\")(gen)\n",
    "gen = keras.layers.BatchNormalization()(gen)\n",
    "gen = LeakyReLU(0.2)(gen)\n",
    "gen = keras.layers.SpatialDropout2D(0.3)(gen)\n",
    "gen = keras.layers.Conv2D(64, (3,3), kernel_initializer = \"random_normal\", padding = \"same\")(gen)\n",
    "gen = keras.layers.BatchNormalization()(gen)\n",
    "gen = LeakyReLU(0.2)(gen)\n",
    "gen = keras.layers.SpatialDropout2D(0.3)(gen)\n",
    "gen = keras.layers.UpSampling2D(size = (2,2), interpolation = \"bilinear\")(gen)\n",
    "# gen = keras.layers.GaussianNoise(0.01)(gen)\n",
    "gen = keras.layers.Conv2D(64, (3,3), kernel_initializer = \"random_normal\", padding = \"same\")(gen)\n",
    "gen = keras.layers.BatchNormalization()(gen)\n",
    "gen = LeakyReLU(0.2)(gen)\n",
    "gen = keras.layers.SpatialDropout2D(0.3)(gen)\n",
    "gen = keras.layers.Conv2D(32, (3,3), kernel_initializer = \"random_normal\", padding = \"same\")(gen)\n",
    "gen = keras.layers.BatchNormalization()(gen)\n",
    "gen = LeakyReLU(0.2)(gen)\n",
    "gen = keras.layers.SpatialDropout2D(0.3)(gen)\n",
    "# gen = keras.layers.GaussianNoise(0.01)(gen)\n",
    "gen_out = keras.layers.Conv2D(1, (1,1), activation = keras.activations.tanh, kernel_initializer = \"random_normal\", padding = \"same\")(gen)\n",
    "generator = keras.Model(inputs = gen_in, outputs = gen_out)\n",
    "logger.debug(\"Generator Summary\")\n",
    "generator.summary(print_fn = logger.debug)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Visualization of untrained generator output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f56ad99ac8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"Visualization of untrained generator output\")\n",
    "noise = tf.random.normal(shape = (1, FEAT_DIM))\n",
    "generated_img = generator(noise, training = False)\n",
    "plt.imshow(tf.reshape(generated_img[0], [28,28,1])[:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(\"Defining separate optimizers for discriminator and generator..\")\n",
    "logger.debug(\"Using ADAM optimizer for both generator and discriminator!\")\n",
    "generator_optimizer = keras.optimizers.Adam(lr = 0.0001)\n",
    "discriminator_optimizer = keras.optimizers.Adam(lr = 0.0001)\n",
    "\n",
    "logger.debug(\"Using BinaryCrossentropy for loss function\")\n",
    "cross_entropy = keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "\n",
    "# Storing the mean loss\n",
    "discriminator_loss = tf.keras.metrics.Mean(name='discriminator_loss')\n",
    "discriminator_real_loss = tf.keras.metrics.Mean(name='discriminator_real_loss')\n",
    "discriminator_fake_loss = tf.keras.metrics.Mean(name='discriminator_fake_loss')\n",
    "generator_loss = tf.keras.metrics.Mean(name='generator_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_discriminator_loss(real_pred, fake_pred):\n",
    "#     real_loss = cross_entropy(tf.ones_like(real_pred), real_pred)\n",
    "#     fake_loss = cross_entropy(tf.zeros_like(fake_pred), fake_pred)\n",
    "#     total_loss = real_loss + fake_loss\n",
    "#     return total_loss\n",
    "\n",
    "# def get_generator_loss(fake_pred):\n",
    "#     fake_loss = cross_entropy(tf.ones_like(fake_pred), fake_pred)\n",
    "#     return fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training function for Discriminator\n",
    "# @tf.function\n",
    "# def train(real):\n",
    "#     noise = tf.random.normal(shape = (32, 100))\n",
    "#     with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "#         fake = generator(noise, training = True)\n",
    "#         real_pred = discriminator(real, training = True)\n",
    "# #         real_loss = cross_entropy(tf.ones_like(real_pred), real_pred)\n",
    "#         fake_pred = discriminator(fake, training = True)\n",
    "# #         dis_fake_loss = cross_entropy(tf.zeros_like(fake_pred), fake_pred)\n",
    "# #         gen_fake_loss = cross_entropy(tf.ones_like(fake_pred), fake_pred)\n",
    "# #         total_loss = real_loss + dis_fake_loss\n",
    "#         dis_loss = get_discriminator_loss(real_pred, fake_pred)\n",
    "#         gen_loss = get_generator_loss(fake_pred)\n",
    "#     dis_gradient = dis_tape.gradient(dis_loss, discriminator.trainable_variables)\n",
    "#     gen_gradient = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "#     discriminator_optimizer.apply_gradients(zip(dis_gradient, discriminator.trainable_variables))\n",
    "    \n",
    "#     generator_optimizer.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n",
    "    \n",
    "#     discriminator_loss(dis_loss)\n",
    "#     generator_loss(gen_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for Discriminator\n",
    "@tf.function\n",
    "def train_discriminator(real, std):\n",
    "    noise = tf.random.normal(shape = (BATCH_SIZE, FEAT_DIM))\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake = generator(noise, training = False)\n",
    "        fake += tf.random.normal(shape = tf.shape(fake), stddev = std, dtype = fake.dtype)\n",
    "        real += tf.random.normal(shape = tf.shape(real), stddev = std, dtype = real.dtype)\n",
    "        real_pred = discriminator(real, training = True)\n",
    "        real_loss = cross_entropy(tf.ones_like(real_pred), real_pred)\n",
    "        fake_pred = discriminator(fake, training = True)\n",
    "        fake_loss = cross_entropy(tf.zeros_like(fake_pred), fake_pred)\n",
    "        total_loss = real_loss + fake_loss\n",
    "    gradient = tape.gradient(total_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradient, discriminator.trainable_variables))\n",
    "    \n",
    "    discriminator_loss(total_loss)\n",
    "    discriminator_real_loss(real_loss)\n",
    "    discriminator_fake_loss(fake_loss)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for Generator\n",
    "@tf.function\n",
    "def train_generator(std):\n",
    "    noise = tf.random.normal(shape = (BATCH_SIZE, FEAT_DIM))\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake = generator(noise, training = True)\n",
    "        fake += tf.random.normal(shape = tf.shape(fake), stddev = std, dtype = fake.dtype)\n",
    "        fake_pred = discriminator(fake, training = False)\n",
    "        fake_loss = cross_entropy(tf.ones_like(fake_pred), fake_pred)\n",
    "    gradient = tape.gradient(fake_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradient, generator.trainable_variables))\n",
    "    \n",
    "    generator_loss(fake_loss)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_dir = 'logs/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - GAN Tricks Used: Normalization of Input between -1 and 1, modified loss for generator as proposed in original gan paper, NN layers initialized with gaussian noise, LeakyReLU as activations of hidden units, tanh activation for output of genrator, Batch Normalization, Average Pooling for downsampling, adding annealed noise to input of discriminator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv2d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-6f4cb0f2eb3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mtrain_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mtrain_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mtrain_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger.info(\"GAN Tricks Used: Normalization of Input between -1 and 1, modified loss for generator as proposed in original gan paper, NN layers initialized with gaussian noise, LeakyReLU as activations of hidden units, tanh activation for output of genrator, Batch Normalization, Average Pooling for downsampling, adding annealed noise to input of discriminator\")\n",
    "logger.debug(\"Beginning training..\")\n",
    "std = 0.08\n",
    "for epoch in range(EPOCHS):\n",
    "    for real in train_ds:\n",
    "#         train(real)\n",
    "        train_discriminator(real, std)\n",
    "        if epoch < 100:\n",
    "            train_discriminator(real, std)\n",
    "        train_generator(std)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('Discriminator loss', discriminator_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('Generator loss', generator_loss.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Discriminator real loss\", discriminator_real_loss.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Discriminator fake loss\", discriminator_fake_loss.result(), step=epoch)\n",
    "    logger.debug(\"Epoch: %d - Noise: %f - Discriminator loss: %f - Generator loss: %f\", epoch+1, std, discriminator_loss.result(), generator_loss.result())\n",
    "    discriminator_loss.reset_states()\n",
    "    discriminator_real_loss.reset_states()\n",
    "    discriminator_fake_loss.reset_states()\n",
    "    generator_loss.reset_states()\n",
    "#     if epoch%10 == 0:\n",
    "    fig, ax = plt.subplots(5, 5, figsize = (15, 15))\n",
    "    buf = io.BytesIO()\n",
    "    noise = tf.random.normal(shape = (25, FEAT_DIM))\n",
    "    generated_images = generator(noise, training = False)\n",
    "    generated_images = tf.reshape(generated_images, [25, 28, 28, 1])\n",
    "    for i in range(25):\n",
    "        _ = ax[i//5, i%5].imshow(generated_images[i][:, :, 0], cmap = \"gray\")\n",
    "    plt.savefig(buf, format = \"png\")\n",
    "    plt.close(fig)\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels = 4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.image(\"Generated Images\", image, step = epoch)\n",
    "    if i!= 0 and i%100 == 0:\n",
    "        std /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize = (15, 15))\n",
    "noise = tf.random.normal(shape = (25, FEAT_DIM))\n",
    "generated_images = generator(noise, training = False)\n",
    "generated_images = tf.reshape(generated_images, [25, 28, 28, 1])\n",
    "for i in range(25):\n",
    "    ax[i//5, i%5].imshow(generated_images[i][:, :, 0], cmap = \"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
